version: '3.8'

services:
  # Conteneur pour l'API FastAPI
  api:
    build:
      context: .
      dockerfile: Dockerfile.api
    volumes:
      - ./api:/app/api  # for the api code (dev)
      - ./data/data_base:/app/data/data_base  # for the agencies names and counters database
      - pip_cache_api:/root/.cache/pip
    ports:
      - "8000:8000"
    working_dir: /app
    command: ["uvicorn", "api.app:app", "--host", "0.0.0.0", "--port", "8000"]
    networks:
      - banking_network

  # Conteneur pour le script ETL extract.py
  etl_extract:
    build:
      context: .
      dockerfile: Dockerfile.etl_extract
    volumes:
      - ./extract:/app/extract # for the api code (dev)
      - ./data/data_base:/app/data/data_base  # for the agencies names and counters database
      - ./data/raw:/app/data/raw
      - pip_cache_etl_extract:/root/.cache/pip
    working_dir: /app
    command: ["python3", "-m", "extract.extract"]
    networks:
      - banking_network
#    depends_on:
#      api:
#        condition: service_healthy

  # Conteneur pour le script ETL transform_load.py (PySpark)
  etl_transform_load:
    build:
      context: .
      dockerfile: Dockerfile.etl_transform_load
    volumes:
      - ./transform_load:/app/transform_load  # for the script
      - ./data/data_base:/app/data/data_base  # for the agencies names and counters database
      - ./data/raw:/app/data/raw
      - ./data/filtered:/app/data/filtered
      - pip_cache_etl_transform_load:/root/.cache/pip
      - apt_cache_etl_transform_load:/var/cache/apt
      - ./spark_cache:/opt/spark_cache
    working_dir: /app
    command: [ "spark-submit", "transform_load/transform_load.py" ]
#    command: ["sleep", "infinity"]
    networks:
      - banking_network
#    depends_on:
#      - etl_extract    # not really depends on etl_extract, depends on CSV files extracted in volume data/raw

  # Conteneur pour l'app Streamlit
  web_app:
    build:
      context: .
      dockerfile: Dockerfile.streamlit
    ports:
      - "8501:8501"
    volumes:
      - ./web_app:/app/web_app
      - ./data/filtered:/app/data/filtered
      - pip_cache_web_app:/root/.cache/pip
    networks:
      - banking_network

  # Conteneur pour Airflow
  airflow:
    build:
      context: .
      dockerfile: Dockerfile.airflow
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - pip_cache_airflow:/home/airflow/.cache/pip
    ports:
      - "8080:8080"
    networks:
      - banking_network
    environment:
      - AIRFLOW__CORE__EXECUTOR=SequentialExecutor
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=sqlite:////opt/airflow/airflow.db
      - AIRFLOW__WEBSERVER__WORKERS=1
      - AIRFLOW__API__AUTH_BACKEND=airflow.api.auth.backend.basic_auth


volumes:
  pip_cache_api:
  pip_cache_etl_extract:
  pip_cache_etl_transform_load:
  apt_cache_etl_transform_load:
  pip_cache_web_app:
  pip_cache_airflow:



networks:
  banking_network:
    driver: bridge

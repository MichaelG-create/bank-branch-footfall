***

**Bank Branch Footfall – Data Engineering Project**

This project simulates and tracks visitor footfall in bank branches through an end‑to‑end data pipeline, from synthetic sensor API to interactive analytics.

***

## Project overview

This repository provides a complete data engineering workflow to monitor branch traffic in near real time and analyze historical patterns.
The goal is to support staffing optimization, opening hours decisions, and customer experience improvements based on realistic synthetic data.

***

## Why this project?

In a context where space and staff optimization are critical, this system helps to:

- Anticipate traffic peaks and adapt staffing levels.
- Optimize opening hours based on actual visitor flows.
- Improve customer experience by reducing waiting times.
- Reuse the same approach for other sectors such as retail, museums, or transportation hubs.

This idea came naturally from real‑world experience: the author previously worked in a bank branch and was responsible for organizing the work of two staff members, which highlighted daily scheduling and workload issues that this project aims to address.

***

## Architecture

![workflow-diagram](images/bank-branch-footfall-workflow.png)

This diagram summarizes the overall workflow from data generation (FastAPI) to ETL (PySpark), orchestration (Airflow), and visualization (Streamlit).

The project is organized around four main components.

### 1. FastAPI service

- REST API exposing `/get_visitor_count` to retrieve the number of visitors entering a branch per hour.
- Agencies are described by size, location type, base traffic, and number of counters, stored in a DuckDB database and loaded as `Agency` objects.
- Visitor counts are generated by `VisitorCounter` using deterministic randomness (date, branch, counter id) to produce realistic yet reproducible behavior, including closed days and sensor malfunctions.

[Public live API example](https://bank-branch-footfall.onrender.com/get_visitor_count?date_time=2025-05-29%2009:05&agency_name=Aix_les_bains_1)


### 2. ETL pipeline

- **Extraction:**
  A Python script (`bank_footfall.etl.extract`) calls the FastAPI endpoint, validates responses with Pydantic models, and writes raw CSV files under `data/raw/` (for example `data/raw/footfall_data.csv`).

- **Transformation (PySpark):**
  The PySpark step (`bank_footfall.etl.transform_load`) reads raw CSVs, cleans and normalizes branch names (fuzzy matching with DuckDB as reference), filters invalid counts, aggregates visitors to daily metrics, and enriches them with weekday information and rolling averages.

- **Load:**
  Transformed data is stored in Parquet format in `data/filtered/parquet` for efficient querying and visualization.

### 3. Orchestration with Airflow

- A **production DAG** (`banking_production_pipeline`) runs hourly to extract data and trigger the Spark transformation.
- A **backfill DAG** (`banking_pipeline_backfill`) replays historical executions over a configured time range and periodically runs the transform step to fill gaps after outages.
- A dedicated DAG (`run_containers.py`) demonstrates how to orchestrate Docker containers for the API, ETL, and web app.

### 4. Visualization with Streamlit

- A Streamlit app (`webapp/app.py`) reads the transformed Parquet data via DuckDB.
- The interface allows choosing:
  - A bank branch.
  - An optional counter (sensor).
  - A period (year, month, week, or custom date range).
- Users can switch between interactive charts and raw tabular data to explore branch traffic.

[Public live application](https://bank-branch-footfall.streamlit.app/)


Example dashboard screenshot:

![streamlit-dashboard](images/streamlit-dashboard.png)

***

## Tech stack

- **Backend & simulation:** FastAPI, Python, NumPy, Pydantic, DuckDB.
- **Data processing:** PySpark, Pandas, fuzzywuzzy for string matching.
- **Orchestration:** Apache Airflow (BashOperator, DockerOperator).
- **Storage:** CSV (raw), Parquet (processed), DuckDB (metadata and analytics).
- **Frontend:** Streamlit.
- **Tooling:** uv for dependency management, Uvicorn for serving FastAPI, `src/` layout for the `bank_footfall` package.

***

## Installation and usage (Linux / WSL)

The project is designed to run on Linux (including WSL2 on Windows).
Commands below assume a Bash shell (`bash`, `zsh`) and Python managed via `uv`.

### 1. Clone the repository

```bash
git clone https://github.com/MichaelG-create/bank-branch-footfall.git
cd bank-branch-footfall
```

Replace the URL with your actual Git remote if different.

### 2. Install uv and project dependencies

Install `uv` if you do not already have it:

```bash
pip install --user uv
# or with pipx
# pipx install uv
```

Then install the project dependencies and the package:

```bash
# From the project root
uv sync

# Install the package in editable mode (src/bank_footfall)
uv pip install -e .
```

The application code lives in `src/bank_footfall` and is installed as the `bank_footfall` package.

***

## Running the components

### 3. Run the FastAPI service

From the project root:

```bash
uv run uvicorn api.app:app --reload --host 0.0.0.0 --port 8000
```

- Interactive documentation: `http://127.0.0.1:8000/docs`
- Main local endpoint:

```text
GET /get_visitor_count
  ?date_time=YYYY-MM-DD HH:MM
  &agency_name=Aix_les_bains_1
  &counter_id=0
  &count_unit=visitors
```

Example local call:

```text
http://127.0.0.1:8000/get_visitor_count?date_time=2025-05-29%2009:05&agency_name=Aix_les_bains_1&counter_id=0&count_unit=visitors
```

***

### 4. Extraction step (ETL)

The modern extractor is in `src/bank_footfall/etl/extract.py` and uses centralized configuration (`bank_footfall.config.settings`).

```bash
uv run python -m bank_footfall.etl.extract
```

This command:

- Calls the `/get_visitor_count` endpoint with parameters defined in the extractor configuration.
- Validates and structures the response via Pydantic models.
- Writes a CSV file to:

```text
data/raw/footfall_data.csv
```

The legacy script (`extract_legacy.py`) is kept for reference but the pipeline uses `bank_footfall.etl.extract` by default.

***

### 5. Java prerequisites for PySpark (Linux / WSL)

The **Transform & Load** phase uses PySpark and requires a Java Development Kit (JDK) installed in your Linux / WSL environment.

Typical setup on Ubuntu / Debian (inside Linux or WSL2):

```bash
sudo apt update
sudo apt install -y openjdk-21-jdk
```

Then configure `JAVA_HOME` in your shell profile (e.g. `~/.bashrc` or `~/.zshrc`):

```bash
echo 'export JAVA_HOME=/usr/lib/jvm/java-21-openjdk-amd64' >> ~/.bashrc
echo 'export PATH="$JAVA_HOME/bin:$PATH"' >> ~/.bashrc
source ~/.bashrc
```

Check the configuration:

```bash
java -version
echo "$JAVA_HOME"
```

If you see errors such as:

```text
Java not found and JAVA_HOME environment variable is not set
PySparkRuntimeError: [JAVA_GATEWAY_EXITED]
```

then the JDK or `JAVA_HOME` is not correctly configured in your Linux / WSL environment.

***

### 6. Transform & Load step (PySpark)

Once Java is correctly installed and `JAVA_HOME` is set:

```bash
uv run python -m bank_footfall.etl.transform_load
```

This step:

- Reads the raw CSV `data/raw/footfall_data.csv`.
- Cleans and normalizes the data (agency names with `fuzzywuzzy`, visitors count, counters, and units).
- Aggregates the data, computes rolling metrics, and merges with existing Parquet when needed.
- Writes the transformed dataset (Parquet, and optionally CSV depending on configuration) to:

```text
data/filtered/
```

***

### 7. Run the Streamlit app

Once transformed data is available:

```bash
uv run streamlit run webapp/app.py
```

The application reads data from `data/filtered/` and provides an interactive visualization of bank branch footfall, as shown in the dashboard screenshot.

***

### 8. Airflow (optional – full orchestration)

To orchestrate extraction and transformation in a more realistic environment on Linux / WSL:

```bash
cd airflow
export AIRFLOW_HOME="$(pwd)"
airflow standalone
```

Then, in the Airflow web UI, enable the DAGs:

- `banking_production_pipeline.py`: hourly real‑time pipeline.
- `banking_pipeline_backfill.py`: backfill DAG to reload historical data after outages.

For container‑based orchestration, `run_containers.py` shows how to trigger the ETL and web app via `DockerOperator`.

***

## Docker and docker-compose

A `docker-compose.yml` file is included to run the full stack in containers on Linux (or via Docker Desktop + WSL2).

Typical services:

- **api:** FastAPI service exposing `/get_visitor_count`, with volumes for API code and DuckDB database.
- **etl_extract:** Container running the extraction step and writing raw CSVs to a shared volume.
- **etl_transform_load:** PySpark container reading raw CSVs and writing Parquet to the same shared volume.
- **web_app:** Streamlit application reading Parquet and exposing the dashboard.
- **airflow:** Airflow scheduler and web UI configured for local development with SQLite.

Volumes are mounted so code changes are quickly reflected in the running containers without full rebuilds, making this setup suitable for development.

***

## Configuration

Project configuration is centralized in `src/bank_footfall/config/config.py` through a `Settings` object:

- **Project paths:** Root directory and Python path for the `src` layout.
- **Database:** Database URL (SQLite by default, overridable via `DATABASE_URL`).
- **API:** Host, port, and hot‑reload setting for the FastAPI service.
- **Logging:** Log level.
- **Airflow:** Optional Airflow home path.

Settings can be overridden via environment variables and a `.env` file at the project root.

***

## Tests

Tests are located under `tests/` and cover several aspects:

- Behavior of `Agency` and `VisitorCounter` (open/closed hours, weekday/weekend rules, sensor malfunctions, badly counting sensors).
- Configuration loading and defaults via `Settings`.
- ETL logic: schema, validation, and aggregation of branch footfall.

Run them from the project root, for example:

```bash
pytest
```

***

## Future work

Planned or possible improvements include:

- Incorporating exceptional events (holidays, promotions, weather) into the traffic simulation.
- Adding predictive models on top of the historical time series.
- Implementing alerts and notifications when footfall deviates from expected patterns.
- Building a Grafana dashboard to monitor pipeline health and latency.
- Refining Docker images and deployment scripts for production.
- Deploying the full stack to a cloud provider (for example GCP) for scalability and resilience.

***

## Author

Developed by **Michael Garcia**, data engineer passionate about automated data pipelines and real‑world analytics for decision‑making.
The project is inspired by personal experience working in a bank branch and organizing the work of two staff members, which revealed concrete coordination and scheduling challenges that this solution is designed to tackle.

# ğŸ¦ **Bank Branch Footfall â€“ Data Engineering Project**

## ğŸš€ **PrÃ©sentation du projet**  
Ce projet permet de suivre en temps rÃ©el lâ€™affluence des visiteurs dans des agences bancaires grÃ¢ce Ã  un pipeline de donnÃ©es automatisÃ©. Lâ€™objectif est dâ€™analyser la frÃ©quentation pour aider Ã  lâ€™optimisation des ressources et Ã  la prise de dÃ©cision.  

## ğŸ” **Pourquoi ce projet ?**  
Dans un contexte oÃ¹ lâ€™optimisation des espaces et des effectifs est clÃ©, ce systÃ¨me permet de :  
âœ… **Anticiper les pics dâ€™affluence** pour ajuster le personnel.  
âœ… **Optimiser les horaires dâ€™ouverture** en fonction des flux rÃ©els.  
âœ… **AmÃ©liorer lâ€™expÃ©rience client** en rÃ©duisant les temps dâ€™attente.  
âœ… **Exploiter ces analyses dans dâ€™autres secteurs** comme les commerces, musÃ©es ou transports.  

## ğŸ—ï¸ **Architecture du projet**  
![data-flow-diagram](data-flow-diagram.png)

### 1ï¸âƒ£ **API FastAPI** ğŸŒ  
[API live](https://bank-branch-footfall.onrender.com/get_visitor_count?date_time=2025-05-29%2009:05&agency_name=Aix_les_bains_1)

Une API REST gÃ©nÃ©rant des donnÃ©es sur le nombre de visiteurs entrant dans une agence bancaire par heure.  

### 2ï¸âƒ£ **Pipeline ETL** ğŸ”„  
- **ğŸ“¥ Extraction** : Un script Python rÃ©cupÃ¨re les donnÃ©es via API et les stocke en CSV.  
- **ğŸ› ï¸ Transformation avec Spark** : Nettoyage des donnÃ©es, agrÃ©gation et calcul de moyennes sur les jours prÃ©cÃ©dents.  
- **ğŸ’¾ Chargement** : Stockage des donnÃ©es transformÃ©es au format **Parquet** pour une exploitation rapide et optimisÃ©e.  

### 3ï¸âƒ£ **Orchestration avec Airflow** â³  
- **ğŸ“Œ DAG principal** : Extraction et transformation toutes les heures.  
- **ğŸ”„ DAG de backfill** : Recharge des donnÃ©es aprÃ¨s une panne pour Ã©viter toute perte.  

### 4ï¸âƒ£ **Visualisation avec Streamlit** ğŸ“Š  
[Application live](https://bank-branch-footfall.streamlit.app/)

Une interface interactive oÃ¹ lâ€™utilisateur peut sÃ©lectionner :  
âœ”ï¸ Une agence bancaire  
âœ”ï¸ Un dÃ©tecteur spÃ©cifique  
âœ”ï¸ Une pÃ©riode pour visualiser les flux de visiteurs  

## ğŸ›  **Technologies utilisÃ©es**  
ğŸš€ **FastAPI** â€“ API REST  
ğŸ **Python** â€“ Extraction des donnÃ©es  
âš¡ **Apache Spark** â€“ Traitement et transformation  
ğŸ“¦ **Parquet** â€“ Stockage optimisÃ©
ğŸ¦† **DuckDB** â€“ Chargement de Parquet vers DB   
ğŸ›© **Apache Airflow** â€“ Orchestration du pipeline  
ğŸ“Š **Streamlit** â€“ Visualisation interactive  

## ğŸ“Œ **Installation et utilisation**  

### ğŸš€ **1. Cloner le dÃ©pÃ´t**  
```bash
git clone https://github.com/MichaelG-create/bank-branch-footfall.git
cd bank-branch-footfall
```

### ğŸš€ **1. CrÃ©er un venv et installer le requirements.txt **  
```bash
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

### ğŸŒ **2. Lancer lâ€™API**  
```bash
uvicorn api.app:app --reload
```
ğŸ“ AccÃ¨s Ã  la documentation interactive : [http://127.0.0.1:8000/docs](http://127.0.0.1:8000/docs)  

### ğŸ”„ **3. ExÃ©cuter le pipeline ETL**  
```bash
python3 extract/extract.py  # Extraction des donnÃ©es API
python3 transfrom_load/transfrom_load.py  # Transformation et chargement
```

### â³ **4. Lancer Airflow**  
DÃ©marrer Airflow et activer les DAGs pour lâ€™orchestration.  
```bash
airflow standalone
```
Les DAGS : 
- banking_pipeline.py : DAG de fonctionnement en temps rÃ©el
- banking_pipeline_back.py : DAG de backfill

### ğŸ“Š **5. Lancer Streamlit**  
```bash
streamlit run webapp/app.py
```
ğŸ“ Interface de visualisation des donnÃ©es en temps rÃ©el.  

## ğŸ”® **Perspectives et amÃ©liorations futures**  
ğŸ”¹ **Prise en compte des Ã©vÃ©nements exceptionnels** (jours fÃ©riÃ©s, promotions, mÃ©tÃ©o).  
ğŸ”¹ **Ajout dâ€™un modÃ¨le de prÃ©diction** pour anticiper les flux.  
ğŸ”¹ **IntÃ©gration dâ€™une base distribuÃ©e** (Delta Lake, BigQuery) pour une scalabilitÃ© accrue.  
ğŸ”¹ **Mise en place dâ€™alertes et notifications** en cas dâ€™affluence anormale.  

## ğŸ‘¤ **Auteur**  
DÃ©veloppÃ© par Michael Garcia, passionnÃ© par la data engineering et lâ€™automatisation des pipelines de donnÃ©es.

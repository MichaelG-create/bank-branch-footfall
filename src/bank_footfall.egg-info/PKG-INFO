Metadata-Version: 2.4
Name: bank-footfall
Version: 0.1.0
Summary: Data engineering project to track simulated footfall in banking agencies
Author: Michael Garcia
Requires-Python: >=3.9
Description-Content-Type: text/markdown
Requires-Dist: fastapi
Requires-Dist: uvicorn[standard]
Requires-Dist: pandas
Requires-Dist: requests
Requires-Dist: apache-airflow
Requires-Dist: pyspark
Requires-Dist: duckdb
Requires-Dist: streamlit
Requires-Dist: pyarrow
Requires-Dist: pydantic>=2.0.0
Requires-Dist: python-dotenv
Requires-Dist: fuzzywuzzy>=0.18.0
Requires-Dist: pydantic-settings>=2.11.0
Requires-Dist: python-levenshtein>=0.27.1
Provides-Extra: dev
Requires-Dist: pytest; extra == "dev"
Requires-Dist: ruff; extra == "dev"
Requires-Dist: pre-commit; extra == "dev"

# ğŸ¦ **Bank Branch Footfall â€“ Data Engineering Project**

## ğŸš€ **PrÃ©sentation du projet**  
Ce projet permet de suivre en temps rÃ©el lâ€™affluence des visiteurs dans des agences bancaires grÃ¢ce Ã  un pipeline de donnÃ©es automatisÃ©. Lâ€™objectif est dâ€™analyser la frÃ©quentation pour aider Ã  lâ€™optimisation des ressources et Ã  la prise de dÃ©cision.  

## ğŸ” **Pourquoi ce projet ?**  
Dans un contexte oÃ¹ lâ€™optimisation des espaces et des effectifs est clÃ©, ce systÃ¨me permet de :  
âœ… **Anticiper les pics dâ€™affluence** pour ajuster le personnel.  
âœ… **Optimiser les horaires dâ€™ouverture** en fonction des flux rÃ©els.  
âœ… **AmÃ©liorer lâ€™expÃ©rience client** en rÃ©duisant les temps dâ€™attente.  
âœ… **Exploiter ces analyses dans dâ€™autres secteurs** comme les commerces, musÃ©es ou transports.  

## ğŸ—ï¸ **Architecture du projet**  
![data-flow-diagram](data-flow-diagram.png)

### 1ï¸âƒ£ **API FastAPI** ğŸŒ  
[API live](https://bank-branch-footfall.onrender.com/get_visitor_count?date_time=2025-05-29%2009:05&agency_name=Aix_les_bains_1)

Une API REST gÃ©nÃ©rant des donnÃ©es sur le nombre de visiteurs entrant dans une agence bancaire par heure.  

### 2ï¸âƒ£ **Pipeline ETL** ğŸ”„  
- **ğŸ“¥ Extraction** : Un script Python rÃ©cupÃ¨re les donnÃ©es via API et les stocke en CSV.  
- **ğŸ› ï¸ Transformation avec Spark** : Nettoyage des donnÃ©es, agrÃ©gation et calcul de moyennes sur les jours prÃ©cÃ©dents.  
- **ğŸ’¾ Chargement** : Stockage des donnÃ©es transformÃ©es au format **Parquet** pour une exploitation rapide et optimisÃ©e.  

### 3ï¸âƒ£ **Orchestration avec Airflow** â³  
- **ğŸ“Œ DAG principal** : Extraction et transformation toutes les heures.  
- **ğŸ”„ DAG de backfill** : Recharge des donnÃ©es aprÃ¨s une panne pour Ã©viter toute perte.  

### 4ï¸âƒ£ **Visualisation avec Streamlit** ğŸ“Š  
[Application live](https://bank-branch-footfall.streamlit.app/)

Une interface interactive oÃ¹ lâ€™utilisateur peut sÃ©lectionner :  
âœ”ï¸ Une agence bancaire  
âœ”ï¸ Un dÃ©tecteur spÃ©cifique  
âœ”ï¸ Une pÃ©riode pour visualiser les flux de visiteurs  

## ğŸ›  **Technologies utilisÃ©es**  
ğŸš€ **FastAPI** â€“ API REST  
ğŸ **Python** â€“ Extraction des donnÃ©es  
âš¡ **Apache Spark** â€“ Traitement et transformation  
ğŸ“¦ **Parquet** â€“ Stockage optimisÃ©
ğŸ¦† **DuckDB** â€“ Chargement de Parquet vers DB   
ğŸ›© **Apache Airflow** â€“ Orchestration du pipeline  
ğŸ“Š **Streamlit** â€“ Visualisation interactive  

Voici une version mise Ã  jour de la section **Installation et utilisation** de ton README, alignÃ©e avec ta nouvelle archi `src/bank_footfall`, uv, FastAPI, lâ€™extractor moderne et PySpark/Java.

Ã€ coller **en remplaÃ§ant** le bloc existant â€œInstallation et utilisationâ€.

***

## ğŸ“Œ Installation et utilisation

### 1ï¸âƒ£ Cloner le dÃ©pÃ´t

```bash
git clone https://github.com/MichaelG-create/bank-branch-footfall.git
cd bank-branch-footfall
```

### 2ï¸âƒ£ CrÃ©er lâ€™environnement avec uv

```bash
# Installer uv si besoin
# pip install uv

# Installer les dÃ©pendances depuis pyproject.toml / uv.lock
uv sync

# Installer le package en mode editable (src/bank_footfall)
uv pip install -e .
```

Le code applicatif vit dans `src/bank_footfall` (layout Â« src Â»).

### 3ï¸âƒ£ Lancer lâ€™API FastAPI

Depuis la racine du projet :

```bash
uv run uvicorn api.app:app --reload
```

- Documentation interactive : http://127.0.0.1:8000/docs  
- Endpoint principal :

```text
GET /get_visitor_count
  ?date_time=YYYY-MM-DD HH:MM
  &agency_name=Aix_les_bains_1
  &counter_id=0
  &count_unit=visitors
```

Exemple local :  
`http://127.0.0.1:8000/get_visitor_count?date_time=2025-05-29%2009:05&agency_name=Aix_les_bains_1&counter_id=0&count_unit=visitors`

### 4ï¸âƒ£ Ã‰tape Extraction (ETL)

Lâ€™extraction moderne est dans `src/bank_footfall/etl/extract.py` et utilise la config centrale (`bank_footfall.config.settings`).

```bash
uv run python -m bank_footfall.etl.extract
```

Cette commande :

- Appelle lâ€™API `/get_visitor_count` avec des paramÃ¨tres configurÃ©s dans `extract.py`.  
- Valide la rÃ©ponse (Pydantic) et la met en forme.  
- Sauvegarde un CSV dans :

```text
data/raw/footfall_data.csv
```

Lâ€™ancien script CLI (`extract_legacy.py`) est conservÃ© pour rÃ©fÃ©rence, mais le pipeline utilise dÃ©sormais `bank_footfall.etl.extract`.

### 5ï¸âƒ£ PrÃ©requis Java pour PySpark

La phase **Transform & Load** utilise PySpark et nÃ©cessite Java.

1. Installer un **JDK** (8 ou 11 recommandÃ©, ex. Temurin/OpenJDK).  
2. DÃ©finir les variables dâ€™environnement (Windows) :

```text
JAVA_HOME = C:\Program Files\Java\jdk-11.0.x
Path += %JAVA_HOME%\bin
```

3. VÃ©rifier :

```bash
java -version
echo %JAVA_HOME%
```

Si tu vois lâ€™erreur :

```text
Java not found and JAVA_HOME environment variable is not set
PySparkRuntimeError: [JAVA_GATEWAY_EXITED]
```

câ€™est que Java/JAVA_HOME ne sont pas correctement configurÃ©s.

### 6ï¸âƒ£ Ã‰tape Transform & Load (PySpark)

Une fois Java opÃ©rationnel :

```bash
uv run python -m bank_footfall.etl.transform_load
```

Cette Ã©tape :

- Lit le CSV brut `data/raw/footfall_data.csv`.  
- Nettoie et corrige les donnÃ©es (fuzzy matching avec `fuzzywuzzy`, Spark transformations).
- Ã‰crit les donnÃ©es transformÃ©es (Parquet / CSV) dans `data/filtered/` selon la config de `transform_load.py`.

Note :  
Un warning peut apparaÃ®tre :

```text
Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning
```

Tu peux lâ€™ignorer, ou accÃ©lÃ©rer `fuzzywuzzy` avec :

```bash
uv add python-Levenshtein
```

### 7ï¸âƒ£ Lancer Streamlit

Une fois les donnÃ©es transformÃ©es disponibles :

```bash
uv run streamlit run webapp/app.py
```

Lâ€™application lit les donnÃ©es dans `data/filtered/` et propose la visualisation interactive du footfall.

### 8ï¸âƒ£ Airflow (optionnel / orchestration complÃ¨te)

Pour orchestrer extraction + transformation en production :

- DÃ©marrer Airflow (par exemple en local) :

```bash
airflow standalone
```

- Activer les DAGs :

  - `banking_pipeline.py` : pipeline horaire temps rÃ©el.
  - `banking_pipeline_backfill.py` : backfill aprÃ¨s panne.

***

ğŸ“ Interface de visualisation des donnÃ©es en temps rÃ©el.  

## ğŸ”® **Perspectives et amÃ©liorations futures**  
ğŸ”¹ **Prise en compte des Ã©vÃ©nements exceptionnels** (jours fÃ©riÃ©s, promotions, mÃ©tÃ©o).  
ğŸ”¹ **Ajout dâ€™un modÃ¨le de prÃ©diction** pour anticiper les flux.  
ğŸ”¹ **Mise en place dâ€™alertes et notifications** en cas dâ€™affluence anormale.  
ğŸ”¹ **CrÃ©ation d'un dashboard avec Grafana** pour suivre l'Ã©tat de santÃ© du pipeline en temps rÃ©el.  
ğŸ”¹ **Conteneurisation** avec Docker pour un dÃ©ploiement facilitÃ© du projet complet.  
ğŸ”¹ **DÃ©ploiement sur le cloud** vers GCP pour une scalabilitÃ© accrue.  


## ğŸ‘¤ **Auteur**  
DÃ©veloppÃ© par Michael Garcia, passionnÃ© par la data engineering et lâ€™automatisation des pipelines de donnÃ©es.
